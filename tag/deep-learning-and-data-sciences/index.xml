<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning and Data Sciences | Chuanfang Ning</title>
    <link>https://chuanfang-ning.github.io/tag/deep-learning-and-data-sciences/</link>
      <atom:link href="https://chuanfang-ning.github.io/tag/deep-learning-and-data-sciences/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning and Data Sciences</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Last update Feb. 2022. Chuanfang Ning Â© 2021-2022 </copyright><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chuanfang-ning.github.io/media/icon_hua7e41f547ede025b028e7da3e44f3d06_120918_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning and Data Sciences</title>
      <link>https://chuanfang-ning.github.io/tag/deep-learning-and-data-sciences/</link>
    </image>
    
    <item>
      <title>(Ongoing) Deep learning method for mobile furniture skeleton localisation </title>
      <link>https://chuanfang-ning.github.io/project/omnibot-skeleton-localisation/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/omnibot-skeleton-localisation/</guid>
      <description>&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/ijspeert/&#34;&gt; Prof. Auke Ijspeert&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/alexandre.alahi?lang=fr&#34;&gt;Prof. Alexandre Alahi&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/anastasia.bolotnikova&#34;&gt;Dr. Anastasia Bolotnikova&lt;/a&gt;, &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/crespi/&#34;&gt;Dr. Alessandro Crespi&lt;/a&gt;, &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Students: Chuanfang Ning, &lt;a href=&#34;https://people.epfl.ch/lixuan.tang&#34;&gt;Lixuan Tang&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;I am working as a Research Assistant at Biorob this semester to continue on my semester project : &lt;a href=&#34;https://chuanfang-ning.github.io/project/omnibot-baseline/&#34;&gt;&amp;ldquo;Omnibot: Mobile Furniture Baseline Development&amp;rdquo;&lt;/a&gt; in collaboration with Lixuan Tang.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are extending the Omnibot design to multiple copies which prepares for a swarm robotics framework.&lt;/li&gt;
&lt;li&gt;We are evaluating our current &lt;a href=&#34;https://openpifpaf.github.io/intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenPifPaf&lt;/a&gt; skeleton localisation model on the Omnibot with the help of key-point localisation with Optitrack system.&lt;/li&gt;
&lt;li&gt;We are facilitating the OpenPifPaf localisation model with real and synthetic data for Omnibot use cases.&lt;/li&gt;
&lt;li&gt;We will improve the OpenPifPaf network structure for furniture localisation based on the test performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To be updated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>(Ongoing) Human-robot tandem race</title>
      <link>https://chuanfang-ning.github.io/project/dl-autonomous/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/dl-autonomous/</guid>
      <description>&lt;div&gt;&lt;font color=&#39;gray&#39;&gt;Course project for &lt;a href=&#34;https://edu.epfl.ch/coursebook/fr/model-predictive-control-ME-425&#34;&gt;CIVIL-459 Deep learning for autonomous vehicles&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Lecturer: &lt;a href=&#34;https://people.epfl.ch/alexandre.alahi?lang=fr&#34;&gt;Prof. Alexandre Alahi&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;Guide an autonomous driving car to react to certain patterns in noisy environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ongoing project.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>U_Cite: America politician network analysis based on Quotebank</title>
      <link>https://chuanfang-ning.github.io/project/ucite/</link>
      <pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/ucite/</guid>
      <description>&lt;div&gt;&lt;font color=&#39;gray&#39;&gt;Final project for &lt;a href=&#34;https://dlab.epfl.ch/teaching/fall2021/cs401/&#34;&gt;CS-401 Applied Data Analysis&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Students: Chuanfang Ning, &lt;a href=&#34;https://github.com/irvin-mero&#34;&gt;Irvin Mero Zambrano&lt;/a&gt;, &lt;a href=&#34;https://github.com/tomcastigl&#34;&gt;Thomas Castiglione&lt;/a&gt;, &lt;a href=&#34;https://github.com/LyKex&#34;&gt;Guoyuan Liu&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Lecturer: &lt;a href=&#34;https://people.epfl.ch/robert.west?lang=en&#34;&gt;Prof. Robert West&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://zenodo.org/record/4277311#.Yf_ZA-rMJm8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quotebank&lt;/a&gt; is a dataset of 235 million unique, speaker-attributed quotations that were extracted from 196 million English news articles crawled from over 377 thousand English web domains. The project aims at analyzing the quotebank mentions in between year 2015 and 2020 to reveal the bi-polar political landscape of America. Keypoints in the project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data cleaning and preprocessing pipeline from original &lt;a href=&#34;https://zenodo.org/record/4277311#.Yf_ZA-rMJm8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quotebank quotations&lt;/a&gt;, &lt;a href=&#34;https://dumps.wikimedia.org/wikidatawiki/entities/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikidata dump&lt;/a&gt; and &lt;a href=&#34;https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QAN5VX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Partisan Audience Bias Scores Dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;political mention analysis pipeline including topic, sentiment and bias analysis.&lt;/li&gt;
&lt;li&gt;political network analysis pipeline including network construction, community/centrality analysis and edge/node feature detection.&lt;/li&gt;
&lt;li&gt;visualisation pipeline for the analysis above with interactive network graphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More details to be found in our &lt;a href=&#34;https://irvin-mero.github.io/date_a_data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; and &lt;a href=&#34;https://irvin-mero.github.io/date_a_data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AutoSynPose: Automatic Generation of Synthetic Datasets for 6D Object Pose Estimation</title>
      <link>https://chuanfang-ning.github.io/project/ue4-dataset/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/ue4-dataset/</guid>
      <description>&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.fh-aachen.de/menschen/kallweit&#34;&gt;Prof. Stephan Kallweit&lt;/a&gt;, &lt;a href=&#34;https://www.fh-aachen.de/menschen/engemann&#34;&gt;Heiko Engemann&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;This project is a continuation to my semester project: 
&lt;a href=&#34;https://chuanfang-ning.github.io/project/ur5-dataset/&#34; title=&#34;Automatic 6D Pose Detection Dataset Capture with UR5 Robot&#34;&gt;Automatic 6D Pose Detection Dataset Capture with UR5 Robot&lt;/a&gt;. We extended our real-world dataset generation pipeline by adding a parallel synthetic dataset generation pipeline, which allows us to improve the performance of our object detection model by combining synthetic and real-world data.&lt;/p&gt;
&lt;p&gt;We used Unreal Engine 4 to generate a 6D pose detection dataset: &lt;a href=&#34;http://autosynpose.fh-aachen.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AutoSynPose&lt;/a&gt; with 6 Mio. subsegments for 5 &lt;a href=&#34;https://www.ycbbenchmarks.com/object-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YCB objects&lt;/a&gt; using 97 rendering locations in 12 different environments.&lt;/p&gt;
&lt;p&gt;This is done by extending the open-source &lt;a href=&#34;https://github.com/NVIDIA/Dataset_Synthesizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA Deep learning Dataset Synthesizer (NDDS)&lt;/a&gt; plugin by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;extending the domain randomization from stochastic distractors to rigid bodies( roughness, specular, metallic and HUE shift).&lt;/li&gt;
&lt;li&gt;extending the stochastic distractors with realistic distractor set (&lt;a href=&#34;https://www.ycbbenchmarks.com/object-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YCB objects&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;adding realistic collision and gravity feature&lt;/li&gt;
&lt;li&gt;adding automatic perspective randomization&lt;/li&gt;
&lt;li&gt;logging domain randomization parameters and enabling clustering of big dataset into sub-datasets accordingly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Demo:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/q2MA7q0aaU0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;More details to be found in our &lt;a href=&#34;http://dx.doi.org/10.3233/FAIA200770&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic 6D Pose Detection Dataset Capture with UR5 Robot</title>
      <link>https://chuanfang-ning.github.io/project/ur5-dataset/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/ur5-dataset/</guid>
      <description>&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; &lt;a href=&#34;https://www.fh-aachen.de/fachbereiche/maschinenbau-und-mechatronik/forschung-projekte/studentische-projekte/pro8projekt2&#34;&gt;Semester project&lt;/a&gt; at FH Aachen, Wintersemester 2019 &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.fh-aachen.de/menschen/kallweit&#34;&gt;Prof. Stephan Kallweit&lt;/a&gt;, &lt;a href=&#34;https://www.fh-aachen.de/menschen/engemann&#34;&gt;Heiko Engemann&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;This project provides an automatic pipeline of creating a real-world 6D pose detection dataset with the help of a wandering UR5 robotic arm on a mobile platform. The 3D model of the training object and its initial offset to the robot base are to be provided. The dataset consits of RGB images, depth images, segmentation masks and 6D poses for the training object.&lt;/p&gt;
&lt;p&gt;This automatic pipeline is mainly targeted at industrial &lt;strong&gt;manipulating&lt;/strong&gt; or &lt;strong&gt;quality inspection&lt;/strong&gt; tasks, where a deep learning model is used to identify/inspect products with &lt;strong&gt;accurate known models&lt;/strong&gt;. The considered application flowchart for the project is shown as in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo1.png&#34; alt=&#34;Demo&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this project, the UR5 robot is mounted on a mobile platform and holds an Intel D435 RGBD camera to wander around the object of interest with the following pattern:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo2.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Robot stop at a place to scan 1/8 sphere space of the training object&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo3.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;At each scanning point the robot turns the sensor to get multi-perspective images&lt;/figcaption&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;The RGB and depth images are captured by the sensor and the ground truth of object&amp;rsquo;s 6D poses, instance/segmentation mask and 3D bounding boxes are calculated from the camera matrix corresponding to robot&amp;rsquo;s joint states in real time.&lt;/p&gt;
&lt;p&gt;Experiment:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo4.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Robot and environment setup&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo5.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Captured dataset samples&lt;/figcaption&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;p&gt;To evaluate the quality of ground truth generated by the method, we annotated the ground truth directly from RGB images once again and checked the BF-Score and IoU of real ground truth with the ground truth generated by our method. The subsegment with YCB bottle got an average score of 0.5156 (BF) and 0.8451 (IoU). The subsegment with Lego car got an average score of 0.8618 (BF) and 0.8551 (IoU).
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo6.png&#34; alt=&#34;Demo&#34; style=&#34;height: 300px;&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;font size=&#39;2&#39;&gt; (Left) Generated ground truth, (Middle) Manually annotated ground truth, (Right) Overlay for error checks&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;Demo:&lt;/p&gt;
&lt;p&gt;A simple show case in Gazebo simulation:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RGKMWWD_0eM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;More details to be found in the &lt;a href=&#34;https://github.com/Chuanfang-Neptune/Realworld_dataset_capturer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
