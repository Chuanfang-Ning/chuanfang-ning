<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechatronics Robotics | Chuanfang Ning</title>
    <link>https://chuanfang-ning.github.io/tag/mechatronics-robotics/</link>
      <atom:link href="https://chuanfang-ning.github.io/tag/mechatronics-robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Mechatronics Robotics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Last update Feb. 2022. Chuanfang Ning Â© 2021-2022 </copyright><lastBuildDate>Tue, 01 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chuanfang-ning.github.io/media/icon_hua7e41f547ede025b028e7da3e44f3d06_120918_512x512_fill_lanczos_center_3.png</url>
      <title>Mechatronics Robotics</title>
      <link>https://chuanfang-ning.github.io/tag/mechatronics-robotics/</link>
    </image>
    
    <item>
      <title>(Ongoing) Deep learning method for mobile furniture skeleton localisation </title>
      <link>https://chuanfang-ning.github.io/project/omnibot-skeleton-localisation/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/omnibot-skeleton-localisation/</guid>
      <description>&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/ijspeert/&#34;&gt; Prof. Auke Ijspeert&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/alexandre.alahi?lang=fr&#34;&gt;Prof. Alexandre Alahi&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/anastasia.bolotnikova&#34;&gt;Dr. Anastasia Bolotnikova&lt;/a&gt;, &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/crespi/&#34;&gt;Dr. Alessandro Crespi&lt;/a&gt;, &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Students: Chuanfang Ning, &lt;a href=&#34;https://people.epfl.ch/lixuan.tang&#34;&gt;Lixuan Tang&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;I am working as a Research Assistant at Biorob this semester to continue on my semester project : &lt;a href=&#34;https://chuanfang-ning.github.io/project/omnibot-baseline/&#34;&gt;&amp;ldquo;Omnibot: Mobile Furniture Baseline Development&amp;rdquo;&lt;/a&gt; in collaboration with Lixuan Tang.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are extending the Omnibot design to multiple copies which prepares for a swarm robotics framework.&lt;/li&gt;
&lt;li&gt;We are evaluating our current furniture skeleton localisation model on the Omnibot with the help of key-point localisation with Optitrack system.&lt;/li&gt;
&lt;li&gt;We are facilitating the skeleton localisation model with real and synthetic data for Omnibot use cases.&lt;/li&gt;
&lt;li&gt;We will improve the network structure based on the test performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To be updated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Omnibot: Mobile furniture baseline development</title>
      <link>https://chuanfang-ning.github.io/project/omnibot-baseline/</link>
      <pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/omnibot-baseline/</guid>
      <description>&lt;p&gt;Semester Project 2021 Fall at &lt;a href=&#34;https://www.epfl.ch/labs/biorob/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BioRob&lt;/a&gt; and &lt;a href=&#34;https://www.epfl.ch/labs/rrl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reconfigurable Robotics Lab&lt;/a&gt; with 6.0/6.0.&lt;/p&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; This project is funded by the CIS Research Pillar, &lt;a href=&#34;https://www.epfl.ch/research/domains/cis/cis-research-pillars/intelligent-assisted-robotics/&#34;&gt;Intelligent Assistive Robotics Grant&lt;/a&gt;. &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/ijspeert/&#34;&gt; Prof. Auke Ijspeert&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/anastasia.bolotnikova&#34;&gt;Dr. Anastasia Bolotnikova&lt;/a&gt;, &lt;a href=&#34;https://www.epfl.ch/labs/biorob/people/crespi/&#34;&gt;Dr. Alessandro Crespi&lt;/a&gt;, &lt;/font&gt;&lt;/div&gt;
&lt;h3 id=&#34;project-goal&#34;&gt;Project Goal&lt;/h3&gt;
&lt;p&gt;The project aims at extending an &lt;a href=&#34;https://www.robotshop.com/en/3wd-100mm-omni-directional-triangle-mobile-robot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Omni-directional Drive Robot Platform&lt;/a&gt; from scratch to a fully functional mobile robot that can drive furniture around according to user needs. The project includes 4 parts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Mechanical part:
&lt;ul&gt;
&lt;li&gt;Design and implement attachment-configuration from the robot platform to various kinds of furniture.&lt;/li&gt;
&lt;li&gt;Extend the platform modules according to user demands (Bluetooth, LED, sensors).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Electronic part:
&lt;ul&gt;
&lt;li&gt;Extend the default board coming with the robot platform from a limited stand-alone system to a system allowing for interactive control and module extension.&lt;/li&gt;
&lt;li&gt;Optimizing the circuit design with our custom PCB.&lt;/li&gt;
&lt;li&gt;Implement the teleoperation of sensors/actuators on the robot platform with ROS_Serial via Bluetooth.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Algorithm part:
&lt;ul&gt;
&lt;li&gt;Real-time localisation with Optitrack.&lt;/li&gt;
&lt;li&gt;Navigation with simplified visibility graph and A*.&lt;/li&gt;
&lt;li&gt;Interactive control including program, voice and gesture interfaces.&lt;/li&gt;
&lt;li&gt;Android application development for mobile furniture remote control.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Project repo archived at &lt;a href=&#34;https://ponyo.epfl.ch/students/mobfur&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;internal Gitlab&lt;/a&gt; (requires EPFL access).&lt;/p&gt;
&lt;p&gt;The Android application control interface with joystick and buttons.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://chuanfang-ning.github.io/media/omnibot_app.png&#34; alt=&#34;Android application&#34; style=&#34;width: 350px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Update 2022-02: Our custom PCB arrives!
&lt;img src=&#34;https://chuanfang-ning.github.io/media/omnibot_pcb.jpg&#34; alt=&#34;Custom PCB&#34; style=&#34;width: 350px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Demo playlists:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/videoseries?list=PLGBGASM3rFjKbd8nlVTlNFSlLiY6I2V4w&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;More details to be found in the &lt;a href=&#34;https://chuanfang-ning.github.io/uploads/Omnibot_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt; and &lt;a href=&#34;https://chuanfang-ning.github.io/uploads/Omnibot_slides.pdf&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Basic mobile robot design with Thymio</title>
      <link>https://chuanfang-ning.github.io/project/thymio-simple/</link>
      <pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/thymio-simple/</guid>
      <description>&lt;div&gt;&lt;font color=&#39;gray&#39;&gt;Final project for &lt;a href=&#34;https://edu.epfl.ch/coursebook/en/basics-of-mobile-robotics-MICRO-452&#34;&gt;MICRO-452 Basics of mobile robotics&lt;/a&gt; with 6.0/6.0&lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Students: Chuanfang Ning, &lt;a href=&#34;https://ch.linkedin.com/in/erfan-etesami&#34;&gt;Erfan Etesami&lt;/a&gt;, &lt;a href=&#34;https://github.com/hmiranda-queiros&#34;&gt; Hugo Miranda Queiros&lt;/a&gt;, &lt;a href=&#34;https://it.linkedin.com/in/laura-boca-de-giuli-0245a3218?trk=public_profile_samename-profile&#34;&gt;Laura Boca de Giuli&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Lecturer: &lt;a href=&#34;http://people.epfl.ch/francesco.mondada&#34;&gt;Prof. Francesco Mondada&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;This is the course project of Basics of mobile robotics. The project goal is to have a minimal but comlpete mobile robot pipeline design on &lt;a href=&#34;https://www.thymio.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thymio robot&lt;/a&gt;. The project covers basic components of a mobile robotic task as shown below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;global mapping with Arcuo markers and HSV&lt;/li&gt;
&lt;li&gt;path planning with A* on visibility graph.&lt;/li&gt;
&lt;li&gt;kalman filter with odometry and camera.&lt;/li&gt;
&lt;li&gt;motion control with waypoints following.&lt;/li&gt;
&lt;li&gt;local avoidance with left-wall following.&lt;/li&gt;
&lt;li&gt;robust to kidnap and unexpected disturbance.&lt;/li&gt;
&lt;li&gt;live visualisation of camera, filtered estimation and uncertainties.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Demo:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/AHRPRVniL0A&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Sim2Real Development for Thymio with ROS</title>
      <link>https://chuanfang-ning.github.io/project/sim2real-thymio/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/sim2real-thymio/</guid>
      <description>&lt;div&gt;&lt;font color=&#39;gray&#39;&gt;Final project for &lt;a href=&#34;https://edu.epfl.ch/coursebook/fr/robotics-practicals-MICRO-453&#34;&gt;MICRO-453 Robotics Practicals&lt;/a&gt; with 5.8/6.0&lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Students: Chuanfang Ning, &lt;a href=&#34;https://github.com/Jianhao-zheng&#34;&gt;Jianhao Zheng&lt;/a&gt;, &lt;a href=&#34;https://github.com/hibetterheyj/&#34;&gt;Yujie He&lt;/a&gt;&lt;/font&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example page&lt;/strong&gt;: &lt;a href=&#34;https://go.epfl.ch/ros_basics_final_2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://go.epfl.ch/ros_basics_final_2021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;â­ &lt;strong&gt;2021/04: Update our project &lt;a href=&#34;https://chuanfang-ning.github.io/uploads/ROS_Basics_Report.pdf&#34; target=&#34;_blank&#34;&gt;Report&lt;/a&gt; !&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;-keywords&#34;&gt;ð Keywords&lt;/h2&gt;
&lt;p&gt;Thymio, PID, Way following, Obstacle avoidance, Pledge algorithm, Aruco marker, Sim2Real, Gazebo&lt;/p&gt;
&lt;h2 id=&#34;-how-to-use&#34;&gt;ð¨ How to use?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;visualize Thymio in RViz&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_exercise thymio_simple_rviz.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;simulate Thymio robot in Gazebo with an interactive window&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_control simu_thymio.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adding waypoints and obstacle for the robot&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_control simu_thymio.launch
roslaunch ros_basics_exercise set_simu_waypoints_obstacle.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tune with rqt tools (rqt_plot, rqt_reconfigrure, rqt_image_show)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_control simu_thymio.launch
roslaunch ros_basics_exercise tune_with_rqt.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;visualize the rosbag files&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_exercise view_with_rosbag.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;extract pose and sensor information from rosbag files&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;roslaunch ros_basics_exercise view_with_rosbag.launch
# open a new terminal
rosrun ros_basics_exercise topic_reader.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;plot trajectory comparison between real and simulation (using matlab)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd results_from_bag/
# run `plot_traj_comp.m` in MATLAB
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./results_from_bag/traj_thymio_simulation_navigation_with_obstacle_avoidance.png&#34; alt=&#34;traj_thymio_simulation_navigation_with_obstacle_avoidance&#34; style=&#34;zoom:30%;&#34; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;connect to the real Thymio&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install thymiodirect
roslaunch ros_basics_control real_thymio.launch
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;example-rosbag-files&#34;&gt;ðExample rosbag files&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;rosbags can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/19KUzVqVasN7F2TfLpSc37OlQIdFQcbJs?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For more info, you can refer to &lt;a href=&#34;./src/ros_basics_exercise/rosbags/readme.md&#34;&gt;&lt;strong&gt;readme.md&lt;/strong&gt; in ./src/ros_basics_exercise/rosbags/&lt;/a&gt; folder.&lt;/p&gt;
&lt;h2 id=&#34;-acknowledgement&#34;&gt;â­ Acknowledgement&lt;/h2&gt;
&lt;p&gt;Thanks to Vaios Papaspyros and Rafael Barmak from MOBOTS at EPFL for the amazing course tutorials !&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improvement on Optobot - an Automated system for Neurogenetics Experimentation</title>
      <link>https://chuanfang-ning.github.io/project/optobot/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/optobot/</guid>
      <description>&lt;p&gt;Semester Project 2020 Fall at &lt;a href=&#34;https://www.epfl.ch/labs/ramdya-lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ramdya Lab (Neuroengineering Laboratory)&lt;/a&gt; with 6.0/6.0.&lt;/p&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://people.epfl.ch/pavan.ramdya?lang=en&#34;&gt;Prof. Pavan Ramdya&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/victor.lobatorios&#34;&gt;Dr. Victor Lobato&lt;/a&gt;, &lt;a href=&#34;https://people.epfl.ch/daniel.morales&#34;&gt;Dr. Daniel Morales&lt;/a&gt;, &lt;/font&gt;&lt;/div&gt;
&lt;h2 id=&#34;project-goal&#34;&gt;Project goal&lt;/h2&gt;
&lt;p&gt;This semester project aims at redesigning, prototyping and programming the carousel, gripper and arena module for the &lt;a href=&#34;https://www.epfl.ch/labs/ramdya-lab/wp-content/uploads/2019/10/Robotic-experimental-automation.mp4?_=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;old Optobot system&lt;/a&gt; to increase its precision on carrying out optogenetic experimentation, which helps to expose Drosophilas to accurately controlled stimuli and record their behaviors. More info on the &lt;a href=&#34;https://www.epfl.ch/labs/ramdya-lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lab page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The redesigned model addressed 3 catastrophic failure modes of the previous design(rotation error, gripping error and returning error) which have made the unsupervised experiments impossible.&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/failure_1.gif&#34; alt=&#34;Demo&#34; style=&#34;height: 250px&#34; /&gt;
&lt;figcaption&gt;Rotation error (misalignment breaks the shelf)&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/failure_2.gif&#34; alt=&#34;Demo&#34; style=&#34;height: 250px&#34;/&gt;
&lt;figcaption&gt;gripping error (invalid data with empty frames)&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/failure_3.gif&#34; alt=&#34;Demo&#34; style=&#34;height: 250px&#34;/&gt;
&lt;figcaption&gt;returning error (remaining container breaks the shelf)&lt;/figcaption&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;h2 id=&#34;project-content&#34;&gt;Project content&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;design and model new carousel, gripper and arena module to resolve the diagnosed failure modes from hardware.&lt;/li&gt;
&lt;li&gt;close the control loop of motor motion with new sensor and increase system precision from software.&lt;/li&gt;
&lt;li&gt;manufacture the prototype of the design in the workshop.&lt;/li&gt;
&lt;li&gt;validate and evolve the design until a functional version.&lt;/li&gt;
&lt;li&gt;adapt control logic and update programs to fit the new design.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;demo&#34;&gt;Demo:&lt;/h2&gt;
&lt;p&gt;Note that the shelf material was out of stock by the semester end. The demo is a show case without shelf. A pink marker is pasted on carousel base to represent the rotating shelves.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/wxcB3_f3D98&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;More details to be found in the &lt;a href=&#34;https://chuanfang-ning.github.io/uploads/Optobot_report.pdf&#34; target=&#34;_blank&#34;&gt;report&lt;/a&gt; and &lt;a href=&#34;https://chuanfang-ning.github.io/uploads/Optobot_slides.pdf&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatic 6D Pose Detection Dataset Capture with UR5 Robot</title>
      <link>https://chuanfang-ning.github.io/project/ur5-dataset/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://chuanfang-ning.github.io/project/ur5-dataset/</guid>
      <description>&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; &lt;a href=&#34;https://www.fh-aachen.de/fachbereiche/maschinenbau-und-mechatronik/forschung-projekte/studentische-projekte/pro8projekt2&#34;&gt;Semester project&lt;/a&gt; at FH Aachen, Wintersemester 2019 &lt;/font&gt;&lt;/div&gt;
&lt;div&gt;&lt;font size=&#39;4&#39; color = &#39;gray&#39;&gt; Supervisor: &lt;a href=&#34;https://www.fh-aachen.de/menschen/kallweit&#34;&gt;Prof. Stephan Kallweit&lt;/a&gt;, &lt;a href=&#34;https://www.fh-aachen.de/menschen/engemann&#34;&gt;Heiko Engemann&lt;/a&gt; &lt;/font&gt;&lt;/div&gt;
&lt;p&gt;This project provides an automatic pipeline of creating a real-world 6D pose detection dataset with the help of a wandering UR5 robotic arm on a mobile platform. The 3D model of the training object and its initial offset to the robot base are to be provided. The dataset consits of RGB images, depth images, segmentation masks and 6D poses for the training object.&lt;/p&gt;
&lt;p&gt;This automatic pipeline is mainly targeted at industrial &lt;strong&gt;manipulating&lt;/strong&gt; or &lt;strong&gt;quality inspection&lt;/strong&gt; tasks, where a deep learning model is used to identify/inspect products with &lt;strong&gt;accurate known models&lt;/strong&gt;. The considered application flowchart for the project is shown as in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo1.png&#34; alt=&#34;Demo&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this project, the UR5 robot is mounted on a mobile platform and holds an Intel D435 RGBD camera to wander around the object of interest with the following pattern:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo2.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Robot stop at a place to scan 1/8 sphere space of the training object&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo3.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;At each scanning point the robot turns the sensor to get multi-perspective images&lt;/figcaption&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;The RGB and depth images are captured by the sensor and the ground truth of object&amp;rsquo;s 6D poses, instance/segmentation mask and 3D bounding boxes are calculated from the camera matrix corresponding to robot&amp;rsquo;s joint states in real time.&lt;/p&gt;
&lt;p&gt;Experiment:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo4.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Robot and environment setup&lt;/figcaption&gt;
&lt;/td&gt;
&lt;td&gt; 
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo5.png&#34; alt=&#34;Demo&#34; style=&#34;height: 170px;&#34;/&gt;
&lt;figcaption&gt;Captured dataset samples&lt;/figcaption&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;Result:&lt;/p&gt;
&lt;p&gt;To evaluate the quality of ground truth generated by the method, we annotated the ground truth directly from RGB images once again and checked the BF-Score and IoU of real ground truth with the ground truth generated by our method. The subsegment with YCB bottle got an average score of 0.5156 (BF) and 0.8451 (IoU). The subsegment with Lego car got an average score of 0.8618 (BF) and 0.8551 (IoU).
&lt;img src=&#34;https://chuanfang-ning.github.io/media/ur5_demo6.png&#34; alt=&#34;Demo&#34; style=&#34;height: 300px;&#34;/&gt;&lt;/p&gt;
&lt;center&gt;&lt;font size=&#39;2&#39;&gt; (Left) Generated ground truth, (Middle) Manually annotated ground truth, (Right) Overlay for error checks&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;Demo:&lt;/p&gt;
&lt;p&gt;A simple show case in Gazebo simulation:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RGKMWWD_0eM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;More details to be found in the &lt;a href=&#34;https://github.com/Chuanfang-Neptune/Realworld_dataset_capturer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
